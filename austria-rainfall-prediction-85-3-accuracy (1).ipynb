{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, LinearRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\n\n# setting seaborn theme\nsns.set_theme(style='whitegrid', palette='ch:.25')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src='https://t1.daumcdn.net/cfile/tistory/99B3263359928F0F30' width='400'>","metadata":{}},{"cell_type":"markdown","source":"# 1. Load Data & Check Information","metadata":{}},{"cell_type":"code","source":"ori = pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')\ndf = pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking Basic Information","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking Correlationship","metadata":{}},{"cell_type":"code","source":"correlation = df.corr()\nplt.figure(figsize=(18,18))\nsns.heatmap(correlation, annot=True, square=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Null Values in each column","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Engineering","metadata":{}},{"cell_type":"markdown","source":"### Dropping Some Columns","metadata":{}},{"cell_type":"markdown","source":"- Since `Evaporation`, `Sunshine`, `Cloud9am`, `Cloud3pm` contain null values more than 30%, it will not help to improve out modes so they will be dropped.\n- `Date` information does not needed, so this will be also dropped.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['Date','Evaporation','Sunshine','Cloud9am','Cloud3pm'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling Null Values\n\nHandling Null Values is really important to predict the answer. <br/>\nIf there is null values in dataset, machine learning model will not accept to fit dataset into model.<br/> Also, it will lead to wrong prediction at the last. \nIn this section, I changed null values by using 2 steps.\n\n1. **Predict null values**<br/> \nJust replacing null values to median or mean would not increase the accuracy of model.<br/> Therefore, by using `KNeighborsRegressor`, I am going to predict missing values as much as possible.\n\n2. **Replace to median**<br/>\nAfter predicting null values, I am going to replace it by using `SimpleImputer`.<br/>\n","metadata":{}},{"cell_type":"markdown","source":"KNN model does not receive **object** type values, so object type values must be replaced to interger of float type.","metadata":{}},{"cell_type":"code","source":"cat_list = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm','RainToday', 'RainTomorrow']\nfor column in cat_list:\n    df[column] = pd.Categorical(df[column])\n    df[column] = df[column].cat.codes\n    #-1 represent NaN in .cat.codes. Therefore, I replaced to real NaN value\n    df[column].replace(-1, np.NaN, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First step will be executed by `filling_null` function. <br/><br/>\nThings going on in `filling_null` :\n* Based on the given feature, it drops all the null variable in other features.\n* Split dataset into train and test set. Train set only include non-null values for given feature, and test set only include null values for given feature.\n* Run KNN model to predict null values\n* Return Dataset","metadata":{}},{"cell_type":"code","source":"def filling_null(feature, df=df):\n    \n    #make train set and test set\n    temp_df = df.copy().drop('RainTomorrow', axis=1)\n    df_list = list(temp_df.columns)\n    df_list.remove(feature)\n    temp_df.dropna(subset=df_list, inplace=True)\n    train = temp_df.loc[temp_df.notna()[feature]]\n    train_x = train.drop(feature, axis=1)\n    train_y = train[feature]\n    test = temp_df[temp_df.isnull()[feature]].drop(feature,axis=1)    \n\n    #run machine learning model and predict null values\n    KNN = KNeighborsRegressor(n_jobs=-1)\n    KNN.fit(train_x, train_y)\n    change_NaN = KNN.predict(test)\n    index_list = test.index.tolist()\n    for i in range(len(change_NaN)):\n        df.at[index_list[i], feature]= change_NaN[i]\n\n    #return dataset which had been changed\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reason why I did not apply all the columns to `filling_null` function is that features not in the list, `Rainfall`, `WindGustSpeed`, `WindDir9am`, `WindDir3pm`, `Temp9am`, `Temp3pm`, `RainToday`, will have empty dataset if I drop null values from other features.","metadata":{}},{"cell_type":"code","source":"apply_list =['MinTemp', 'MaxTemp', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'Humidity9am',\n             'Humidity3pm', 'Pressure9am', 'Pressure3pm']\n\n\nfor feature in apply_list:\n    df = filling_null(feature = feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, `WindDir9am` feature has changed a lot! <br> Other features also changed, but not like `WindDir9am`. However, it is still worth it","metadata":{}},{"cell_type":"code","source":"#getting information of number of null variable changed\ndf_columns = list(df.columns)\nchanged_dict = {}\nfor col in df_columns:\n    changed_dict[\"%s\" %col] = len(df[col].dropna()) - len(ori[col].dropna())\n\n#delet features which did not changed at all\npop_list = ['Location','Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Temp9am','Temp3pm','RainToday','RainTomorrow']\nfor feature in pop_list:\n    changed_dict.pop(feature)\n\n#make list of key and value to visualize the graph\nkey_list = []\nvalue_list = []\nfor key, value in changed_dict.items():\n    key_list.append(key)\n    value_list.append(value)\n\ntemp_df = pd.DataFrame()\ntemp_df['key'] = key_list\ntemp_df['value'] = value_list\n\n#visualization\nplt.figure(figsize=(25, 10))\nplot = sns.barplot(x='key',y='value', data=temp_df)\nfor p in plot.patches:\n    plot.annotate(format(p.get_height(), '0.0f'), \n                   (p.get_x() + p.get_width() / 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.yscale('log')\nplot.axes.get_yaxis().set_visible(False)\nplt.title('# of null values which changed to non-null values', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we finished first step, we are going to do second step, which is **Replace to median**.","metadata":{}},{"cell_type":"code","source":"df_Xnul = df.fillna(df.median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_Xnul.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Train & Test set\nBefore moving on, we need to make train and test set.","metadata":{}},{"cell_type":"code","source":"X = df_Xnul.drop(['RainTomorrow'], axis=1)\ny = df_Xnul['RainTomorrow']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Model Selection & GridSearch\nNow it's time to select best model and do some hyperparameter tuning!","metadata":{}},{"cell_type":"code","source":"classifier_names = [\"Logistic Regression\",'SGDClassifier', \"Random Forest\",\"KNN\",\"Decision\"]\n\nclassifiers = [LogisticRegression(), SGDClassifier(), RandomForestClassifier(), KNeighborsClassifier(), DecisionTreeClassifier()]\n\nzipped_clf = zip(classifier_names,classifiers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classifier(classifier, t_train, c_train, t_test, c_test):\n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([\n            ('standardize', StandardScaler()),                         \n            ('classifier', c)\n        ])\n        print(\"Validation result for {}\".format(n))\n        print(c)\n        clf_acc = fit_classifier(checker_pipeline, t_train, c_train, t_test,c_test)\n        result.append((n,clf_acc))\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_classifier(pipeline, X_train, y_train, X_test, y_test):\n    model_fit = pipeline.fit(X_train, y_train)\n    y_pred = model_fit.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n    print()\n    return accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By applying 5 machine learning models, `Random Forest` gained highest accuracy score(85.29%) with default hyperparameter. </br>\nRandom Forest's hyperparameters will be tuned to increase the accuracy score little bit more.","metadata":{}},{"cell_type":"code","source":"result = classifier(zipped_clf, X_train, y_train, X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_list = {'min_samples_leaf':[3,6,9], 'max_depth': [5,10,None], 'criterion' : ['gini', 'entropy']}\nRFC = RandomForestClassifier(n_jobs=-1, random_state=42)\nclf = GridSearchCV(RFC,param_list)\nclf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.best_estimator_.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test,y_pred)*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reference\n\n* https://towardsdatascience.com/automate-the-machine-learning-model-implementation-with-sklearn-pipeline-2ef1389062c9","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}